{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalakAhmed2003/Cellula/blob/main/Cellula_task_4_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZE9-mqVNM6o8"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive/satalite data/data/images\"\n",
        "!ls \"/content/drive/MyDrive/satalite data/data/labels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPc1xsS-LhFv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "# === Mount Google Drive ===\n",
        "# Mountpoint should be a local directory path, not a URL\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Set Paths ===\n",
        "# Update paths to point to the unzipped data directory\n",
        "image_dir = \"/content/drive/MyDrive/satalite data/data/images\"\n",
        "mask_dir  = \"/content/drive/MyDrive/satalite data/data/labels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEuu3Kp2QzEx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tifffile\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def load_tif_images_and_png_masks(image_dir, mask_dir, img_size=(128, 128), max_channels=12):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # Extract filenames without extensions\n",
        "    image_files = {os.path.splitext(f)[0]: f for f in os.listdir(image_dir) if f.endswith(\".tif\")}\n",
        "    mask_files  = {os.path.splitext(f)[0]: f for f in os.listdir(mask_dir) if f.endswith(\".png\")}\n",
        "\n",
        "    # Use only matching base names\n",
        "    common_keys = sorted(set(image_files.keys()) & set(mask_files.keys()))\n",
        "    print(f\"✅ Found {len(common_keys)} matched image–mask pairs.\")\n",
        "\n",
        "    for key in common_keys:\n",
        "        try:\n",
        "            img_path = os.path.join(image_dir, image_files[key])\n",
        "            mask_path = os.path.join(mask_dir, mask_files[key])\n",
        "\n",
        "            # === Load .tif Image ===\n",
        "            img_array = tifffile.imread(img_path)\n",
        "\n",
        "            if img_array.ndim == 2:\n",
        "                img_array = np.expand_dims(img_array, axis=-1)\n",
        "            elif img_array.shape[0] <= max_channels and img_array.shape[0] < img_array.shape[-1]:\n",
        "                img_array = np.transpose(img_array, (1, 2, 0))\n",
        "            if img_array.shape[-1] > max_channels:\n",
        "                img_array = img_array[..., :max_channels]\n",
        "\n",
        "            img_array = cv2.resize(img_array, img_size)\n",
        "            img_array = img_array.astype(np.float32) / 255.0\n",
        "\n",
        "            # === Load .png Mask ===\n",
        "            mask = Image.open(mask_path).convert(\"L\")  # convert to grayscale\n",
        "            mask = mask.resize(img_size, Image.NEAREST)\n",
        "            mask_array = np.array(mask)\n",
        "            mask_array = (mask_array > 0).astype(np.uint8)  # binary mask\n",
        "\n",
        "            images.append(img_array)\n",
        "            masks.append(mask_array)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Skipping {key}: {e}\")\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"❌ No matching image–mask pairs were loaded.\")\n",
        "\n",
        "    return np.stack(images), np.stack(masks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGJYZ2_cQ6Hd"
      },
      "outputs": [],
      "source": [
        "image_dir = \"/content/drive/MyDrive/satalite data/data/images\"\n",
        "mask_dir  = \"/content/drive/MyDrive/satalite data/data/labels\"\n",
        "X_data, Y_data = load_tif_images_and_png_masks(image_dir, mask_dir)\n",
        "\n",
        "# Optional: split into train/val/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X_data, Y_data, test_size=0.3, random_state=42)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F2Ve-XLRokh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_sample(X, Y, index=0):\n",
        "    image = X[index]\n",
        "    mask = Y[index]\n",
        "\n",
        "    if image.shape[-1] > 3:\n",
        "        image = image[..., :3]\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(\"Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_sample(X_data, Y_data, index=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C08QRVlPQS0_"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "output_dir = \"converted_png\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for path in image_dir:\n",
        "    try:\n",
        "        img = tiff.imread(path)\n",
        "        img = (img / img.max() * 255).astype(np.uint8)\n",
        "        out_path = os.path.join(output_dir, os.path.basename(path).replace(\".tif\", \".png\"))\n",
        "        Image.fromarray(img).save(out_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not convert {path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ikUHDW2TQp9R"
      },
      "outputs": [],
      "source": [
        "import tifffile as tiff\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path =  \"/content/drive/MyDrive/satalite data/data/images\"\n",
        "\n",
        "try:\n",
        "    img = tiff.imread(path)\n",
        "    print(f\"✅ Loaded: shape={img.shape}, dtype={img.dtype}\")\n",
        "\n",
        "    # Normalize for viewing\n",
        "    img = img.astype(np.float32)\n",
        "    img -= img.min()\n",
        "    img /= img.max() if img.max() > 0 else 1\n",
        "\n",
        "    plt.imshow(img if img.ndim == 2 else img[..., 0], cmap='gray')\n",
        "    plt.title(\"Preview of 58.tif\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MeKDDXdRJtn"
      },
      "outputs": [],
      "source": [
        "import tifffile as tiff\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def load_and_normalize_tif(path):\n",
        "    img = tiff.imread(path).astype(np.float32)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    img -= img.min()\n",
        "    if img.max() > 0:\n",
        "        img /= img.max()\n",
        "\n",
        "    # Ensure 3D shape: (H, W, C)\n",
        "    if img.ndim == 2:\n",
        "        img = np.expand_dims(img, axis=-1)\n",
        "    return img\n",
        "\n",
        "def load_and_prepare_dataset_tiff(image_paths, label_paths):\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for img_path, lbl_path in tqdm(zip(image_paths, label_paths), total=len(image_paths)):\n",
        "        try:\n",
        "            img = load_and_normalize_tif(img_path)\n",
        "            lbl = np.array(Image.open(lbl_path)).astype(np.int64)\n",
        "\n",
        "            X.append(img)\n",
        "            Y.append(lbl)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping {img_path}: {e}\")\n",
        "\n",
        "    return np.array(X), np.array(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ifJJCAb7Rzzm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_batch(images, masks, num=4):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for i in range(num):\n",
        "        # Image\n",
        "        plt.subplot(2, num, i + 1)\n",
        "        img = images[i]\n",
        "\n",
        "        if img.shape[-1] == 1:\n",
        "            plt.imshow(img[:, :, 0], cmap='gray')\n",
        "        elif img.shape[-1] >= 3:\n",
        "            plt.imshow(img[:, :, :3])  # show first 3 bands\n",
        "        else:\n",
        "            raise ValueError(f\"Image has unsupported shape: {img.shape}\")\n",
        "\n",
        "        plt.title(f\"Image {i}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Mask\n",
        "        plt.subplot(2, num, i + 1 + num)\n",
        "        plt.imshow(masks[i], cmap='gray')\n",
        "        plt.title(f\"Mask {i}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show first 4\n",
        "show_batch(X_train, Y_train, num=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "508f2388"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation-models-pytorch efficientnet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WUDVwGHpUr4x"
      },
      "outputs": [],
      "source": [
        "# 📦 Install if needed\n",
        "!pip install segmentation_models_pytorch --quiet\n",
        "\n",
        "# 🧠 Load Pretrained U-Net\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 🔧 Model configuration\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",        # Encoder backbone\n",
        "    encoder_weights=\"imagenet\",     # Use ImageNet pretrained weights\n",
        "    in_channels=12,                  # RGB input\n",
        "    classes=1,                      # Binary mask output (1 channel)\n",
        "    activation=None                 # We'll apply sigmoid manually during inference\n",
        ")\n",
        "\n",
        "# Move model to GPU or CPU\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"✅ Pretrained U-Net model loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMSNGMVP0QLI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models import resnet50\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Helper: Create dataloader from NumPy arrays ===\n",
        "def create_dataloader(X, Y, batch_size=8, shuffle=False):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2)  # NHWC → NCHW\n",
        "    Y_tensor = torch.tensor(Y, dtype=torch.long)  # Masks as integer labels\n",
        "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "# === Dataloaders ===\n",
        "train_loader = create_dataloader(X_train, Y_train, batch_size=8, shuffle=True)\n",
        "val_loader   = create_dataloader(X_val, Y_val, batch_size=8, shuffle=False)\n",
        "test_loader  = create_dataloader(X_test, Y_test, batch_size=8, shuffle=False)\n",
        "\n",
        "# === Load Pretrained DeepLabV3 ===\n",
        "weights = DeepLabV3_ResNet50_Weights.DEFAULT\n",
        "model = deeplabv3_resnet50(weights=weights)\n",
        "\n",
        "# === Modify first conv layer to accept 12-channel input ===\n",
        "# Old: Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "old_conv = model.backbone.conv1\n",
        "new_conv = nn.Conv2d(12, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Copy weights from original 3 channels, others = mean\n",
        "with torch.no_grad():\n",
        "    new_conv.weight[:, :3] = old_conv.weight\n",
        "    if new_conv.weight.shape[1] > 3:\n",
        "        mean_weights = old_conv.weight.mean(dim=1, keepdim=True)\n",
        "        new_conv.weight[:, 3:] = mean_weights.repeat(1, 9, 1, 1)  # Fill remaining channels\n",
        "\n",
        "model.backbone.conv1 = new_conv\n",
        "\n",
        "# === Modify classifier for 2-class output ===\n",
        "model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)\n",
        "model = model.to(device)\n",
        "\n",
        "# === Loss & Optimizer ===\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# === Training Loop ===\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)['out']  # [B, C, H, W]\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # === Validation ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"→ Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
        "\n",
        "# === Testing ===\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)['out']\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(masks.numpy())\n",
        "\n",
        "# Flatten predictions and ground truth\n",
        "y_true = np.concatenate([y.flatten() for y in all_labels])\n",
        "y_pred = np.concatenate([y.flatten() for y in all_preds])\n",
        "\n",
        "# === Classification Report ===\n",
        "print(\"\\n✅ Water Class Metrics:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Non-Water\", \"Water\"], zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X-EiyRz8BY1"
      },
      "outputs": [],
      "source": [
        "!pip install nbstripout\n",
        "# Running nbstripout on the current notebook from within the notebook might not work as intended.\n",
        "# If your goal is to clear outputs, use the Colab interface (Edit -> Clear all outputs).\n",
        "!nbstripout Cellula_task_4.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgQCNwqU8rki"
      },
      "outputs": [],
      "source": [
        "!ls Cellula_task_4.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7i-X46w89zq"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "\n",
        "notebook_path = \"/content/drive/MyDrive/Colab Notebooks/Cellula_task_4.ipynb\"\n",
        "output_path = notebook_path.replace(\".ipynb\", \"_clean.ipynb\")\n",
        "\n",
        "with open(notebook_path) as f:\n",
        "    nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "# Remove problematic metadata\n",
        "if 'widgets' in nb['metadata']:\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Save clean notebook\n",
        "with open(output_path, 'w') as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(f\"✅ Cleaned notebook saved to: {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO5ZiI9sZuV/cRJBheHAjJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}